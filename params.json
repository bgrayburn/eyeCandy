{"name":"eyeCandy","tagline":"Collaborative Visual Generation using Processing, Node, and OSC","body":"# what is it\r\n\r\n![example1](https://raw.githubusercontent.com/humanwireio/eyeCandy/master/exampleImages/example1.png)\r\n\r\n\r\n\r\n\r\n###imagineâ€¦\r\nYou're in a room with thirty other people.\r\nYour phone is in your hand and your browser is open to a website, as is every other person's. The exact page (selected from a root directory page) determines what part of a complex visual on the wall you are controlling. If you leave that page the pattern you were controlling before disappears and will be replaced by a new pattern once you navigate to a new page.\r\n\r\nYou can try it out right now on your computer!\r\n\r\n####donate\r\n\r\nIf you like what you see here, help me fund this puppy\r\n\r\n####types of interaction\r\n\r\nBecause processing and node have libraries galore, it's easy to add almost any type of interaction.\r\n\r\nBy default we use [jquery-kontrol](http://anthonyterrien.com/kontrol/) for xypads, sliders, knobs but any library that can be dropped in will work (think webcams, accelerometers, etc.)\r\n\r\n# how it works\r\nThe website you are accessing is being run on a node.js server, which in turn is sending open sound\r\n control (OSC) messages to a processing sketch telling it when you visit pages and interact with elements of the site.\r\n In response the processing sketch produces sweet visuals based on this information like this!\r\n\r\nBecause the OSC server (processing) can receive messages from anyone, multiple devices can control different, or the same elements of the visuals being generated.  You're only limited by how many devices you can connect to it.\r\n\r\n# dependencies\r\nInstall these guys first:\r\n\r\n* [node](http://nodejs.org/), npm\r\n* [processing](https://processing.org/download/) (2.2.1+)\r\n  * [oscP5](http://www.sojamo.de/libraries/oscP5/)\r\n  * [minim](http://code.compartmental.net/tools/minim/) (for FFTCircles patch)\r\n  * processing-java command line (installable from tools menu in processing ide)\r\n* screen (optional, can just run webserver in the background using &)\r\n\r\n# how to use\r\nThese directions assume both the node.js webserver server and processing are running on the same machine. \r\n\r\n* check the ip address (you'll need that soon) `ifconfig`\r\n* clone the repo using\r\n`git clone https://github.com/humanwireio/eyeCandy/`\r\n* start the node server\r\n```\r\ncd eyeCandy;\r\nscreen -d -m node webserver/server.js\r\n```\r\n\r\n* start processing\r\n```\r\nprocessing-java viz/viz.pde\r\n```\r\n\r\nDone setting up now lets use it!\r\n\r\n* Type the ip address of the computer running the node server in a browser on any device capable of pinging that computer (probably on the local network).\r\n* As you navigate the pages you should see the image change in processing. \r\n\r\n\r\n# contributing and bugs\r\nFeel free to make pull requests or drop a line to [humanwire.io@gmail.com](mailto://humanwire.io@gmail.com) regarding this code. I plan to work on standardizing the osc addresses to work for any patch, cleaning up the folder structure, and creating a config file and parser. I'll also be updating some webcam code to create an example of a patch generated from a cam.\r\n\r\n\r\n# credits\r\n* [node-osc-kontrol](https://github.com/TheAlphaNerd/node-osc-kontrol)\r\n* [mathographics](http://www.amazon.com/Mathographics-Robert-Dixon/dp/B00AK2VKNO/ref=sr_1_1?ie=UTF8&qid=1408638440&sr=8-1&keywords=mathographics)\r\n","google":"UA-54088807-1","note":"Don't delete this file! It's used internally to help with page regeneration."}